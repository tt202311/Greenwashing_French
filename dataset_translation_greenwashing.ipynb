{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/QwenLM/Qwen2.5?tab=readme-ov-file\n",
        "\n",
        "https://qwenlm.github.io/blog/qwen2.5/\n"
      ],
      "metadata": {
        "id": "9ZCFEhpsu4Hg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPFVi0UFur4a"
      },
      "outputs": [],
      "source": [
        "# Load model directly\n",
        "!pip install transformers\n",
        "!pip install accelerate\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\n",
        "\n",
        "# Check for GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-3B\")\n",
        "model = AutoModel.from_pretrained(\"Qwen/Qwen2.5-3B\").to(device)  # Move model to GPU"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "splits = {\n",
        "    'train': 'data/train-00000-of-00001-98aa5228a06a17d0.parquet',\n",
        "    'validation': 'data/validation-00000-of-00001-2553e47d408fab28.parquet',\n",
        "    'test': 'data/test-00000-of-00001-79fd931297fff765.parquet'\n",
        "}\n",
        "df = pd.read_parquet(\"hf://datasets/climatebert/environmental_claims/\" + splits[\"validation\"])\n",
        "\n",
        "def translate_text(text):\n",
        "    # Use model to translate\n",
        "    inputs = tokenizer(\"Translate to French: \" + text, return_tensors=\"pt\").to(device)  # Move input tensors to GPU\n",
        "    outputs = model.generate(**inputs)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "U7_S5hsSwcb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "cNloVtTw1gGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply translation on dataset\n",
        "df['translated_text'] = df['text'].apply(translate_text)  # replace 'text_column_name' with actual text column name\n",
        "\n",
        "# Save translated claims to a CSV file\n",
        "df[['text_column_name', 'translated_text']].to_csv('translated_claims.csv', index=False)\n",
        "\n",
        "print(\"Translated claims saved to 'translated_claims.csv'\")"
      ],
      "metadata": {
        "id": "5opBjTkIwehf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset analisys"
      ],
      "metadata": {
        "id": "7PBiMTLswEQy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "!pip install pyarrow  # Required if reading Parquet files"
      ],
      "metadata": {
        "id": "7K1KxoGWwH_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "\n",
        "# Download NLTK data files (only need to run once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "splits = {\n",
        "    'train': 'data/train-00000-of-00001-98aa5228a06a17d0.parquet',\n",
        "    'validation': 'data/validation-00000-of-00001-2553e47d408fab28.parquet',\n",
        "    'test': 'data/test-00000-of-00001-79fd931297fff765.parquet'\n",
        "}\n",
        "\n",
        "# Choose the split you want to analyze (e.g., 'train', 'validation', 'test')\n",
        "split_to_analyze = 'train'\n",
        "\n",
        "# Load the selected dataset split\n",
        "df = pd.read_parquet(\"hf://datasets/climatebert/environmental_claims/\" + splits[split_to_analyze])"
      ],
      "metadata": {
        "id": "3dt4mifVwVbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_column = 'text'\n",
        "\n",
        "# Ensure the text column exists\n",
        "if text_column not in df.columns:\n",
        "    raise ValueError(f\"The specified text column '{text_column}' does not exist in the dataset.\")\n"
      ],
      "metadata": {
        "id": "gG8-ff3Aw0gG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to count sentences and tokens in a text\n",
        "def analyze_text(text):\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    return len(sentences), len(tokens)\n",
        "\n",
        "# Apply the function to each row in the dataset\n",
        "df[['sentence_count', 'token_count']] = df[text_column].apply(\n",
        "    lambda text: pd.Series(analyze_text(text))\n",
        ")\n",
        "\n",
        "# Calculate total counts\n",
        "total_sentences = df['sentence_count'].sum()\n",
        "total_tokens = df['token_count'].sum()\n",
        "\n",
        "print(f\"Total number of sentences in the '{split_to_analyze}' dataset: {total_sentences}\")\n",
        "print(f\"Total number of tokens in the '{split_to_analyze}' dataset: {total_tokens}\")"
      ],
      "metadata": {
        "id": "rQLGRH6Bw-Fw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Translation DeepL"
      ],
      "metadata": {
        "id": "poQc6-nTxMi7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deepl\n",
        "!pip install pyarrow  # If not already installed\n",
        "!pip install tqdm"
      ],
      "metadata": {
        "id": "hHBlD6cUxLoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import deepl\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "os.environ['DEEPL_AUTH_KEY'] = ''\n",
        "\n",
        "auth_key = os.getenv('DEEPL_AUTH_KEY')\n",
        "\n",
        "# Check if the API key is available\n",
        "if auth_key is None:\n",
        "    raise ValueError(\"Please set the DEEPL_AUTH_KEY environment variable.\")\n",
        "\n",
        "translator = deepl.Translator(auth_key)"
      ],
      "metadata": {
        "id": "hCCk_qNNxc-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translator"
      ],
      "metadata": {
        "id": "I3QUj6-r3xAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "splits = {\n",
        "    'train': 'data/train-00000-of-00001-98aa5228a06a17d0.parquet',\n",
        "    'validation': 'data/validation-00000-of-00001-2553e47d408fab28.parquet',\n",
        "    'test': 'data/test-00000-of-00001-79fd931297fff765.parquet'\n",
        "}\n",
        "\n",
        "split_to_translate = 'validation'  # Change this to 'validation' or 'test' as needed\n",
        "\n",
        "df = pd.read_parquet(\"hf://datasets/climatebert/environmental_claims/\" + splits[split_to_translate])\n",
        "\n",
        "# Replace 'text_column_name' with the actual name of the text column in your dataset\n",
        "text_column = 'text'  # e.g., 'claim_text' or 'description'\n",
        "\n",
        "# Ensure the text column exists\n",
        "if text_column not in df.columns:\n",
        "    raise ValueError(f\"The specified text column '{text_column}' does not exist in the dataset.\")"
      ],
      "metadata": {
        "id": "l41vlo2dxd6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "U5yWjLcHxwvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('original_validation_dataset.csv', index=False)"
      ],
      "metadata": {
        "id": "yVAvCzbcmEUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Copy Columns**"
      ],
      "metadata": {
        "id": "n2wJp9womraQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# copy column\n",
        "import pandas as pd\n",
        "\n",
        "# File paths\n",
        "masked_training_file = \"/content/translated_claims_validation.csv\"\n",
        "output_file_final = \"/content/original_validation_dataset.csv\"\n",
        "\n",
        "# Load datasets\n",
        "masked_training_df = pd.read_csv(masked_training_file)\n",
        "output_file_final_df = pd.read_csv(output_file_final)\n",
        "\n",
        "# Check if 'label' column exists in output file\n",
        "if 'label' in output_file_final_df.columns:\n",
        "    # Copy the 'label' column from the output file to the masked training dataset\n",
        "    masked_training_df['label'] = output_file_final_df['label']\n",
        "    # Save the updated dataset\n",
        "    masked_training_df.to_csv(masked_training_file, index=False)\n",
        "    print(f\"'label' column copied and saved to {masked_training_file}.\")\n",
        "else:\n",
        "    print(\"'label' column not found in the output file.\")"
      ],
      "metadata": {
        "id": "kR-WlZ5zmqs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_texts(texts):\n",
        "    try:\n",
        "        # Use DeepL API to translate a list of texts\n",
        "        results = translator.translate_text(texts, target_lang='FR')\n",
        "        return [result.text for result in results]\n",
        "    except deepl.DeepLException as e:\n",
        "        print(f\"Error translating batch: {e}\")\n",
        "        return [None] * len(texts)"
      ],
      "metadata": {
        "id": "Re-mC41hxzPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch processing parameters\n",
        "batch_size = 50  # Adjust the batch size based on your needs and API limitations\n",
        "\n",
        "# Prepare a list to store translated texts\n",
        "translated_texts = []\n",
        "\n",
        "# Total number of texts to translate\n",
        "total_texts = len(df)\n",
        "\n",
        "# Use tqdm to create a progress bar\n",
        "for start_idx in tqdm(range(0, total_texts, batch_size), desc=\"Translating\", unit=\"batch\"):\n",
        "    end_idx = min(start_idx + batch_size, total_texts)\n",
        "    batch_texts = df[text_column].iloc[start_idx:end_idx].tolist()\n",
        "    translated_batch = translate_texts(batch_texts)\n",
        "    translated_texts.extend(translated_batch)\n",
        "\n",
        "# Add the translated texts to the DataFrame\n",
        "df['translated_text'] = translated_texts\n",
        "\n",
        "# Save translated claims to a CSV file\n",
        "output_file = f'translated_claims_{split_to_translate}.csv'\n",
        "df[[text_column, 'translated_text']].to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"Translated claims saved to '{output_file}'\")"
      ],
      "metadata": {
        "id": "JbSJOFtpx6RS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save Original Dataset"
      ],
      "metadata": {
        "id": "1BH230m594ZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the columns you want to save\n",
        "columns_to_save = ['text', 'label', 'translated_text']  # Replace with your actual column names\n",
        "\n",
        "# Save the DataFrame to a CSV file with the specified columns\n",
        "df.to_csv('output_file.csv', columns=columns_to_save, index=False)\n",
        "\n",
        "print(\"Data saved to 'output_file.csv' with columns:\", columns_to_save)\n"
      ],
      "metadata": {
        "id": "E3rXNg_98ttL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validation\n",
        "\n",
        "English model should classify French claims.\n",
        "\n",
        "Model: https://huggingface.co/climatebert/environmental-claims\n",
        "\n",
        "Dataset: https://huggingface.co/datasets/climatebert/environmental_claims\n"
      ],
      "metadata": {
        "id": "YeVOtp8c3-ew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers pandas torch scikit-learn"
      ],
      "metadata": {
        "id": "qAGpr-X5ClMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
        "\n",
        "# Load the translated data\n",
        "translated_data_file = 'output_file.csv'  # Replace with your actual file path\n",
        "df = pd.read_csv(translated_data_file)\n",
        "\n",
        "# Ensure the translated column and label column exist\n",
        "translated_column = 'translated_text'  # Column with French claims\n",
        "label_column = 'label'  # Column with the original English labels (adjust as needed)\n",
        "\n",
        "if translated_column not in df.columns or label_column not in df.columns:\n",
        "    raise ValueError(\"Ensure the translated and label columns are present in the DataFrame.\")"
      ],
      "metadata": {
        "id": "_7E-RqC-Cng1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "-l5sh41KC0Zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained ClimateBERT model for classification\n",
        "model_name = \"climatebert/environmental-claims\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Check if CUDA (GPU) is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "hKYeOjn3C_jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the French claims\n",
        "def tokenize_claims(claims):\n",
        "    return tokenizer(claims, truncation=True, padding=True, return_tensors='pt')\n",
        "\n",
        "# Classify the claims\n",
        "def classify_claims(translated_claims):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for claim in translated_claims:\n",
        "            inputs = tokenize_claims([claim]).to(device)\n",
        "            outputs = model(**inputs)\n",
        "            preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
        "            predictions.extend(preds)\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# Get predictions for the translated French claims\n",
        "translated_claims = df[translated_column].tolist()\n",
        "predicted_labels = classify_claims(translated_claims)\n",
        "\n",
        "# Add predictions to the DataFrame\n",
        "df['predicted_label'] = predicted_labels\n",
        "\n",
        "# Calculate metrics\n",
        "true_labels = df[label_column].tolist()\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Precision, Recall, F1-score\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predicted_labels, average='weighted')\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "# Detailed classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(true_labels, predicted_labels, target_names=['0', '1']))\n",
        "\n",
        "# Save the DataFrame with predictions\n",
        "output_file = 'validated_translations_with_metrics.csv'\n",
        "df.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"Validated translations saved to '{output_file}'\")"
      ],
      "metadata": {
        "id": "NGgdrVzWDIQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The destribution of claims in original dataset :\n",
        "\n",
        "Class 0: 1585 samples\n",
        "\n",
        "Class 1: 532 samples\n",
        "\n",
        "Results of applying English climateBert model on translated to french claims  (To sum up - it detects 0 claims better because the dataset imbalanced):\n",
        "\n",
        "Accuracy: 0.7520\n",
        "\n",
        "Precision: 0.7474\n",
        "\n",
        "Recall: 0.7520\n",
        "\n",
        "F1-Score: 0.6522\n",
        "\n",
        "\n",
        "Classification Report:\n",
        "\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.75      1.00      0.86      1585\n",
        "           1       0.73      0.02      0.04       532\n",
        "\n",
        "    accuracy                           0.75      2117\n",
        "   macro avg       0.74      0.51      0.45      2117\n",
        "weighted avg       0.75      0.75      0.65      2117\n",
        "\n",
        "\n",
        "Observations\n",
        "Class 0 (Majority Class):\n",
        "\n",
        "Precision: 0.75 means that 75% of predictions for class 0 are correct.\n",
        "Recall: 1.00 means the model correctly identifies all instances of class 0.\n",
        "F1-Score: 0.86 indicates excellent performance for class 0.\n",
        "Conclusion: The model performs well for the majority class.\n",
        "Class 1 (Minority Class):\n",
        "\n",
        "Precision: 0.73 means that when the model predicts class 1, it is correct 73% of the time.\n",
        "Recall: 0.02 is extremely low, indicating that the model identifies only 2% of actual class 1 instances.\n",
        "F1-Score: 0.04 reflects poor performance in balancing precision and recall for class 1.\n",
        "Conclusion: The model struggles significantly to detect class 1 instances.\n",
        "\n",
        "Key Issues\n",
        "Class Imbalance:\n",
        "\n",
        "There is a significant imbalance in the data:\n",
        "Class 0: 1585 instances (75%)\n",
        "Class 1: 532 instances (25%)\n",
        "The model heavily favors the majority class (class 0), which explains the high recall for class 0 but very low recall for class 1.\n",
        "Low Recall for Class 1:\n",
        "\n",
        "The recall for class 1 is only 0.02, meaning the model fails to identify most actual instances of class 1.\n",
        "This is problematic, especially if class 1 is critical for your application (e.g., detecting environmental claims).\n",
        "F1-Score Disparity:\n",
        "\n",
        "The F1-score for class 1 (0.04) is much lower than for class 0 (0.86), highlighting the model's inability to balance precision and recall for the minority class."
      ],
      "metadata": {
        "id": "KIoexOFfVgWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "from transformers import AutoTokenizer\n",
        "import pandas as pd\n",
        "\n",
        "# Load the ClimateBERT tokenizer\n",
        "model_name = \"climatebert/environmental-claims\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Load the translated dataset\n",
        "translated_data_file = 'output_file_final.csv'  # Replace with your dataset path\n",
        "df = pd.read_csv(translated_data_file)\n",
        "\n",
        "# Define the columns for references and candidate translations\n",
        "reference_column = 'text'  # Original English sentences\n",
        "candidate_column = 'translated_text'  # Translated French sentences\n",
        "\n",
        "# Ensure the columns exist\n",
        "if reference_column not in df.columns or candidate_column not in df.columns:\n",
        "    raise ValueError(f\"Ensure '{reference_column}' and '{candidate_column}' columns exist in the DataFrame.\")\n",
        "\n",
        "# Function to preprocess and tokenize text\n",
        "def preprocess_and_tokenize(text, lang):\n",
        "    # Preprocess text: lowercasing and stripping unnecessary whitespace\n",
        "    text = text.lower().strip()\n",
        "    # Tokenize using the ClimateBERT tokenizer\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    return tokens\n",
        "\n",
        "# Prepare tokenized references and candidates\n",
        "references = [[preprocess_and_tokenize(ref, 'en')] for ref in df[reference_column]]  # Tokenize references\n",
        "candidates = [preprocess_and_tokenize(cand, 'fr') for cand in df[candidate_column]]  # Tokenize candidates\n",
        "\n",
        "# Calculate corpus-level BLEU score\n",
        "smooth_fn = SmoothingFunction().method4  # Smoothing for better BLEU scores\n",
        "bleu_score = corpus_bleu(references, candidates, smoothing_function=smooth_fn)\n",
        "\n",
        "# Print the BLEU score\n",
        "print(f\"Corpus BLEU Score: {bleu_score:.4f}\")"
      ],
      "metadata": {
        "id": "HJopsJavbM_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "eYKoBk2xbynf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Translate Test Dataset"
      ],
      "metadata": {
        "id": "RVszz6im0-H3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import deepl\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set DeepL API key\n",
        "os.environ['DEEPL_AUTH_KEY'] = ''\n",
        "\n",
        "# Get authentication key from environment variable\n",
        "auth_key = os.getenv('DEEPL_AUTH_KEY')\n",
        "if auth_key is None:\n",
        "    raise ValueError(\"Please set the DEEPL_AUTH_KEY environment variable.\")\n",
        "\n",
        "# Initialize DeepL translator\n",
        "translator = deepl.Translator(auth_key)\n",
        "\n",
        "# Define dataset paths\n",
        "splits = {\n",
        "    'train': 'data/train-00000-of-00001-98aa5228a06a17d0.parquet',\n",
        "    'validation': 'data/validation-00000-of-00001-2553e47d408fab28.parquet',\n",
        "    'test': 'data/test-00000-of-00001-79fd931297fff765.parquet'\n",
        "}\n",
        "\n",
        "# Load the test dataset\n",
        "test_split = 'test'\n",
        "df = pd.read_parquet(\"hf://datasets/climatebert/environmental_claims/\" + splits[test_split])\n",
        "\n",
        "# Define text and label column names\n",
        "text_column = 'text'  # Update this if necessary\n",
        "label_column = 'label'  # Ensure this matches the dataset\n",
        "\n",
        "# Ensure the necessary columns exist\n",
        "if text_column not in df.columns:\n",
        "    raise ValueError(f\"The specified text column '{text_column}' does not exist in the dataset.\")\n",
        "if label_column not in df.columns:\n",
        "    df[label_column] = None  # Assign None if the label column is missing\n",
        "\n",
        "# Function to translate text in batches\n",
        "def translate_texts(texts):\n",
        "    try:\n",
        "        results = translator.translate_text(texts, target_lang='FR')\n",
        "        return [result.text for result in results]\n",
        "    except deepl.DeepLException as e:\n",
        "        print(f\"Error translating batch: {e}\")\n",
        "        return [\"Error\"] * len(texts)\n",
        "\n",
        "# Batch translation parameters\n",
        "batch_size = 50  # Adjust based on API limits\n",
        "translated_texts = []\n",
        "total_texts = len(df)\n",
        "\n",
        "# Translate in batches with progress tracking\n",
        "for start_idx in tqdm(range(0, total_texts, batch_size), desc=\"Translating\", unit=\"batch\"):\n",
        "    end_idx = min(start_idx + batch_size, total_texts)\n",
        "    batch_texts = df[text_column].iloc[start_idx:end_idx].tolist()\n",
        "    translated_batch = translate_texts(batch_texts)\n",
        "    translated_texts.extend(translated_batch)\n",
        "\n",
        "# Add translations to DataFrame\n",
        "df['translated_text'] = translated_texts\n",
        "\n",
        "# Ensure translation column is not empty\n",
        "if df['translated_text'].isnull().all():\n",
        "    raise ValueError(\"Translation failed. Check DeepL API settings and retry.\")\n",
        "\n",
        "# Save translated dataset with labels (if available)\n",
        "output_file = f'translated_claims_{test_split}.csv'\n",
        "df[[text_column, 'translated_text', label_column]].to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"Translated claims saved to '{output_file}'\")"
      ],
      "metadata": {
        "id": "7WDg0VNjpPRm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}